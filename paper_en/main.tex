\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{amssymb}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}

\usepackage{orcidlink}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\crefname{figure}{Fig.}{Figs.}

\usepackage{xcolor}
\definecolor{BestRed}{rgb}{0.8,0,0}
\newcommand{\best}[1]{\textcolor{BestRed}{#1}}

\begin{document}

\title{Adaptive Wavelet Convolution for Vision Tasks: Coordinate Gating, Strip Frequency Gating, and Pixel-wise Fusion}

\author{\textbf{[Author Name(s) Placeholder]}
\thanks{\textbf{[Affiliation and contact Placeholder]}}}

\maketitle

\begin{abstract}
Convolutional neural networks are widely used in object detection, semantic segmentation, and image restoration. However, purely relying on fixed-scale local convolutions often struggles to preserve fine textures while maintaining global structures. Wavelet transforms provide a natural multi-scale and interpretable band decomposition, explicitly separating low-frequency structures from high-frequency details, which facilitates frequency-domain modeling. Existing wavelet convolutions typically adopt relatively static depthwise processing or simple re-scaling on subbands, lacking adaptive control over spatial positions, directional frequency responses, and cross-branch interactions. As a result, the contribution of each frequency band is difficult to adjust dynamically across samples and regions.

We propose an interface-compatible adaptive wavelet convolution module, termed AWTConv2d. While keeping the wavelet analysis/synthesis scaffold unchanged, AWTConv2d introduces three key mechanisms. First, it performs per-channel learnable subband mixing via grouped $1\times1$ transforms, enabling adaptive reorganization among $\{LL, LH, HL, HH\}$. Second, it integrates coordinate gating and strip frequency gating to modulate subband features conditioned on spatial locations and directional low/high-frequency components. Third, it replaces simple summation with pixel-wise adaptive fusion to softly select between the spatial convolution branch and the wavelet reconstruction branch, improving complementary information aggregation.

Experiments on \textbf{[Dataset Placeholder: e.g., COCO / DOTA / VisDrone / custom dataset]} show that AWTConv2d consistently improves \textbf{[Metric Placeholder: mAP / AP50 / mIoU / PSNR, etc.]} with minimal changes to the overall network architecture, and ablation studies verify the contribution of each component.
\end{abstract}

\begin{IEEEkeywords}
Wavelet transform, wavelet convolution, frequency-domain modeling, coordinate gating, attention mechanism, feature fusion
\end{IEEEkeywords}

\input{content/introduction}
\input{content/related_work}
\input{content/method}
\input{content/experiments}
\input{content/conclusion}

\bibliographystyle{IEEEtran}
\bibliography{ref}
\end{document}
