\section{Experiments}
\label{sec:exp}

This section describes the experimental setup, compared methods, and result analysis. Since our contribution is primarily a plug-and-play module replacement, we keep the backbone, training schedule, and data processing pipeline unchanged as much as possible, and only replace the designated convolution modules to evaluate the general performance gain of AWTConv2d.

\subsection{Experimental Setup}
\paragraph{Datasets and metrics}
We evaluate on \textbf{[Dataset Placeholder: e.g., COCO/DOTA/VisDrone/custom dataset]}. For object detection, we report \textbf{[Metric Placeholder: mAP@0.5:0.95, AP50, AP75]}; for segmentation, \textbf{[Metric Placeholder: mIoU]}; and for restoration, \textbf{[Metric Placeholder: PSNR/SSIM]}. The dataset split and evaluation protocol follow \textbf{[Protocol Placeholder: official split/custom split]}.

\paragraph{Networks and replacement strategy}
We choose \textbf{[Backbone/Detector Placeholder: e.g., RT-DETR/Deformable DETR/YOLO family]} as the baseline. The replacement strategy is to substitute the original depthwise convolution or WTConv2d with AWTConv2d at \textbf{[Location Placeholder: e.g., specific depthwise conv layers in the backbone, certain conv layers in the neck]}, while keeping the rest of the architecture unchanged. For fair comparison, all hyperparameters other than the replaced module remain the same.

\paragraph{Training details}
We train with \textbf{[Optimizer Placeholder: SGD/AdamW]}, an initial learning rate of \textbf{[lr Placeholder]}, weight decay \textbf{[wd Placeholder]}, batch size \textbf{[bs Placeholder]}, and \textbf{[epoch Placeholder]} epochs. Data augmentation includes \textbf{[Augmentation Placeholder: multi-scale, random crop, mixup, etc.]}. Experiments are conducted on \textbf{[Hardware Placeholder: GPU model and memory]}.

\subsection{Compared Methods}
We compare against the baseline convolution (Depthwise Conv), WTConv2d~\cite{wtconv2024}, and several plug-and-play attention/fusion modules (e.g., coordinate gating~\cite{coordgate2024}, strip attention~\cite{fsa2024}, and pixel-wise fusion~\cite{cga2024}). All comparisons use the same training schedule and inference settings.

\subsection{Main Results}
\Cref{tab:main-results} reports the main results on \textbf{[Dataset Placeholder]}. Under controlled parameter and computation changes, AWTConv2d consistently improves over both the baseline and WTConv2d. This indicates that coordinate gating, strip frequency gating, and pixel-wise fusion effectively enhance the adaptivity of the wavelet branch and the complementarity between the two branches.

\begin{table}[t]
\centering
\caption{Main results on \textbf{[Dataset Placeholder]} (numbers are placeholders and should be filled with actual results).}
\label{tab:main-results}
\begin{tabular}{lccc}
	oprule
Method & Params (M) & FLOPs (G) & \textbf{[Metric Placeholder]} \\
\midrule
Baseline (DWConv) & [\#] & [\#] & [\#] \\
WTConv2d~\cite{wtconv2024} & [\#] & [\#] & [\#] \\
AWTConv2d (Ours) & [\#] & [\#] & \best{[\#]} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablation Study}
To analyze the contribution of each component, we progressively add the key designs of AWTConv2d while keeping all other settings fixed. \Cref{tab:ablation} presents the ablation results. Overall, subband mixing and subband attention provide a robust baseline gain; coordinate gating further improves location-dependent adaptivity; strip frequency gating is particularly helpful for samples with directional textures and edge structures; and pixel-wise fusion significantly improves local complementarity between the spatial branch and the wavelet branch, leading to the best overall performance.

\begin{table}[t]
\centering
\caption{Ablation of AWTConv2d components (numbers are placeholders and should be filled with actual results).}
\label{tab:ablation}
\begin{tabular}{lcccc}
	oprule
Setting & SubbandMix & CoordGate & StripGate & \textbf{[Metric Placeholder]} \\
\midrule
A0: WT scaffold + subband DWConv &  &  &  & [\#] \\
A1: + subband mixing & \checkmark &  &  & [\#] \\
A2: + coordinate gating & \checkmark & \checkmark &  & [\#] \\
A3: + strip frequency gating & \checkmark & \checkmark & \checkmark & [\#] \\
A4: + pixel-wise fusion (final) & \checkmark & \checkmark & \checkmark & \best{[\#]} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Qualitative Analysis and Discussion (Optional)}
To better understand the behavior of the module, we suggest visualizing subband attention weights, the spatial distribution of coordinate gates, and the heatmap of the pixel fusion gate $\mathbf{P}$. For detection, one may further group samples by small objects, elongated objects, and complex backgrounds to examine the impact of strip frequency gating on directional textures. Such visualizations can be included in \textbf{[Visualization Placeholder: figure id / appendix location]}.

\subsection{Information Needed to Replace Placeholders}
To replace placeholders with publishable experimental numbers, please confirm the dataset name and split, the baseline model and configuration path, the exact layers where the module is replaced, training hyperparameters (lr/epochs/batch size), and the computation/parameter counting protocol (e.g., which script/tool is used to report FLOPs/Params).
