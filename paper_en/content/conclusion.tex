\section{Conclusion}
\label{sec:conclusion}

This paper proposes an adaptive wavelet convolution module, AWTConv2d, targeting plug-and-play integration of wavelet-based operators for vision tasks. While preserving the invertible wavelet analysis/synthesis scaffold, AWTConv2d improves adaptivity from both subband processing and cross-branch fusion. In the wavelet domain, we introduce per-channel learnable subband mixing and subband attention to enable content-adaptive reorganization among $LL/LH/HL/HH$. We further incorporate coordinate gating for position-conditioned modulation, and employ strip frequency gating to explicitly model directional low/high-frequency components. Finally, we adopt pixel-wise adaptive fusion to softly select between the spatial branch and the wavelet reconstruction branch at a fine granularity, mitigating the interference caused by naive summation.

Experimental results on \textbf{[Dataset Placeholder]} demonstrate that AWTConv2d provides consistent performance gains with a small additional computational cost, and ablation studies verify the effectiveness of each component. Future work may proceed in two directions. First, we will explore stronger forms of dynamic frequency filtering, such as low-rank or interpolatable frequency weights that generalize across resolutions. Second, we will extend the proposed adaptive mechanisms to more general multi-branch architectures and multimodal tasks, further evaluating robustness and interpretability in complex real-world scenarios.
