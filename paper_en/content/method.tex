\section{Method}
\label{sec:method}

This section first reviews the basic form of wavelet convolution and then details the proposed adaptive wavelet convolution module, AWTConv2d. The module is designed to remain interface-compatible with WTConv2d, so it can be used as a drop-in replacement for depthwise separable convolutions or depthwise convolutions in existing detection/segmentation networks.

\subsection{Preliminaries: 2D Discrete Wavelet Analysis and Synthesis}
Given an input feature map $\mathbf{X}\in\mathbb{R}^{B\times C\times H\times W}$, the 2D discrete wavelet transform can be viewed as a downsampling convolution with a set of fixed analysis filters. The output consists of one low-frequency subband and three high-frequency subbands. We denote wavelet analysis as
\begin{equation}
\mathbf{U}=\mathcal{W}(\mathbf{X})\in\mathbb{R}^{B\times C\times 4\times \frac{H}{2}\times \frac{W}{2}},
\end{equation}
where $\mathbf{U}_{:,:,0,:,:}$ corresponds to the $LL$ subband and $\mathbf{U}_{:,:,1:4,:,:}$ corresponds to the three high-frequency subbands $LH/HL/HH$. Wavelet synthesis (inverse transform) is denoted as
\begin{equation}
\hat{\mathbf{X}}=\mathcal{W}^{-1}(\mathbf{U})\in\mathbb{R}^{B\times C\times H\times W}.
\end{equation}
In implementation, $\mathcal{W}$ and $\mathcal{W}^{-1}$ can be realized by grouped convolution and transposed convolution, respectively. The filters are determined by the chosen wavelet basis (e.g., db1/db2) and are kept fixed.

\subsection{Baseline: Subband Processing and Fusion in WTConv2d}
The basic pipeline of WTConv2d performs multi-level wavelet decomposition, applies depthwise convolution on subband features, reconstructs features through inverse transforms level by level, and finally adds the reconstruction to a spatial depthwise convolution branch~\cite{wtconv2024}. Its key advantage is an explicit band pathway with invertible reconstruction. However, the subband processing is usually static (depthwise convolution plus fixed scaling), and the fusion is a simple summation, lacking input-adaptive selection mechanisms.

\subsection{AWTConv2d: Adaptive Subband Token Mixer}
AWTConv2d keeps the wavelet pyramid scaffold but upgrades the per-level subband processing into a composite operator consisting of local refinement, learnable subband mixing, spatial/directional modulation, and subband attention. For the wavelet output at level $l$, denoted as $\mathbf{U}^{(l)}$, we first reshape it into
\begin{equation}
\mathbf{T}^{(l)}\in\mathbb{R}^{B\times 4C\times H_l\times W_l},
\end{equation}
where $H_l=H/2^l$ and $W_l=W/2^l$.

\paragraph{(1) Local refinement within each subband}
We apply depthwise convolution to $\mathbf{T}^{(l)}$ to capture local patterns inside each subband, denoted as $\phi_{dw}(\cdot)$.

\paragraph{(2) Per-channel learnable subband mixing}
To enable information exchange among $\{LL,LH,HL,HH\}$ within the same channel, we introduce a grouped $1\times1$ transform $\phi_{mix}(\cdot)$ with groups set to $C$. This learns a $4\rightarrow 4$ linear mixing inside each channel. Compared with mixing across all $4C$ channels, this design avoids unconstrained cross-channel coupling, improving stability and interpretability.

\paragraph{(3) Coordinate gating: position-conditioned band modulation}
The dependence on low-/high-frequency information typically varies across spatial locations. For example, boundary regions may rely more on high-frequency details, while large smooth background regions may rely more on low-frequency structures. Inspired by CoordGate~\cite{coordgate2024}, we use the normalized coordinate grid $(x,y)\in[-1,1]^2$ as conditioning input, and employ a lightweight $1\times1$ convolutional network to produce a pixel-wise gate
$\mathbf{G}_{coord}^{(l)}\in(0,1)^{B\times 4C\times H_l\times W_l}$. Then we modulate the features by
\begin{equation}
	ilde{\mathbf{T}}^{(l)} = \mathbf{T}^{(l)} \odot \mathbf{G}_{coord}^{(l)}.
\end{equation}
This implementation does not depend on a fixed input resolution and thus can be applied across stages and varying input sizes.

\paragraph{(4) Strip frequency gating: directional low/high-frequency re-organization}
To explicitly model directional textures and elongated structures, we introduce strip-based low/high decomposition~\cite{fsa2024}. Concretely, we perform strip average pooling along the horizontal and vertical directions to obtain low-frequency components, and compute high-frequency components as residuals. Learnable coefficients then mix low/high components and are injected back in a residual manner. Denoting this operator as $\psi_{strip}(\cdot)$, we have
\begin{equation}
\bar{\mathbf{T}}^{(l)} = \psi_{strip}(\tilde{\mathbf{T}}^{(l)}).
\end{equation}
Unlike relying solely on convolution kernels to implicitly learn directional responses, this mechanism provides structured control over low-frequency smoothing and high-frequency edge emphasis.

\paragraph{(5) Subband attention: content-adaptive reweighting across subbands}
After subband mixing and spatial/directional modulation, we apply subband attention to $\bar{\mathbf{T}}^{(l)}$. The attention extracts global statistics via global average pooling, and uses a grouped $1\times1$ transform with groups=$C$ to generate four subband weights per channel, enabling content-adaptive selection among $LL/LH/HL/HH$.

Overall, the level-$l$ subband processing can be summarized as
\begin{equation}
\mathbf{T}^{(l)} \leftarrow \alpha\big(\psi_{strip}(\phi_{coord}(\phi_{mix}(\phi_{dw}(\mathbf{T}^{(l)}))))\big),
\end{equation}
where $\alpha(\cdot)$ denotes the combination of subband attention and scaling.

\subsection{Wavelet Reconstruction and Pixel-wise Cross-branch Fusion}
After processing $L$ levels of subbands, we perform top-down inverse transforms to reconstruct the wavelet branch output $\mathbf{X}_{wave}$. In parallel, the spatial branch produces $\mathbf{X}_{base}$ using depthwise convolution.

Prior approaches often fuse branches by a direct sum $\mathbf{X}_{base}+\mathbf{X}_{wave}$, but the reliability of the two branches can vary across regions. Therefore, we introduce a pixel-wise fusion module. Inspired by content-guided attention fusion~\cite{cga2024}, we jointly use channel attention and spatial attention to generate a pixel gate $\mathbf{P}\in(0,1)^{B\times C\times H\times W}$, and perform per-pixel soft fusion:
\begin{equation}
\mathbf{X}_{fuse} = \mathbf{X}_{init} + \mathbf{P}\odot\mathbf{X}_{base} + (1-\mathbf{P})\odot\mathbf{X}_{wave},\quad \mathbf{X}_{init}=\mathbf{X}_{base}+\mathbf{X}_{wave}.
\end{equation}
In addition, we keep a conservative channel-wise gate $\sigma(\mathbf{g})$ to modulate the magnitude of $\mathbf{X}_{wave}$ before fusion, improving training stability when the wavelet branch is initially under-optimized. The final output uses a $1\times1$ projection to match the channel dimension required by the downstream network.

\subsection{Complexity and Plug-and-play Discussion}
Compared with WTConv2d, the added overhead of AWTConv2d mainly comes from the grouped $1\times1$ subband mixing, the lightweight coordinate gating network, strip pooling operations, and the depthwise $7\times7$ convolution used in pixel-wise fusion. All components can be efficiently implemented with standard deep learning operators without custom CUDA kernels. Meanwhile, AWTConv2d preserves the same input/output interface as WTConv2d, allowing straightforward replacement and ablation in existing codebases.
